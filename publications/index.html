<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Abhijit Kundu | publications</title>
    <meta name="author" content="Abhijit Kundu" />
    <meta name="description" content="My research publications." />
    <meta name="keywords" content="computer-vision, robotics, machine-learning, 3D-vision, academic-website" />

    <!-- OpenGraph -->
    <meta property="og:site_name" content="Abhijit Kundu" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Abhijit Kundu | publications" />
    <meta property="og:url" content="https://abhijitkundu.info/publications/" />
    <meta property="og:description" content="My research publications." />
    
    <meta property="og:locale" content="en" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="publications" />
    <meta name="twitter:description" content="My research publications." />
    
    <meta name="twitter:site" content="@_abhijit_kundu_" />
    <meta name="twitter:creator" content="@_abhijit_kundu_" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://abhijitkundu.info/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://abhijitkundu.info/"><span class="font-weight-bold">Abhijit</span>   Kundu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description">My research publications.</p>
          </header>

          <article>
            <p>Also see my <a href="https://scholar.google.com/citations?user=GGcjtCsAAAAJ&amp;h" target="_blank" rel="noopener noreferrer">google scholar</a> <i class="ai ai-google-scholar ai-1x"></i> page.
<!-- _pages/publications.md --></p>
<div class="publications">
  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://omninocs.github.io/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/omninocs/omninocs_video_teaser_1x2.webp"></a></div>

        <!-- Entry bib key -->
        <div id="OmniNOCSECCV2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects</div>
          <!-- Author -->
          <div class="author">
<a href="https://akshay-krishnan.github.io/" target="_blank" rel="noopener noreferrer">Akshay Krishnan</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.kmaninis.com/" target="_blank" rel="noopener noreferrer">Kevis-Kokitsi Maninis</a>,Â <a href="https://faculty.cc.gatech.edu/~hays/" target="_blank" rel="noopener noreferrer">James Hays</a>,Â <a href="https://mattabrown.github.io/" target="_blank" rel="noopener noreferrer">Matthew Brown.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ECCV</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2407.08711" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2407.08711.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://github.com/google-deepmind/omninocs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code <i class="fab fa-github"></i></a>
            <a href="https://omninocs.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized Object Coordinate Space (NOCS) maps, object masks, and 3D bounding box annotations for indoor and outdoor scenes. OmniNOCS has 20 times more object classes and 200 times more instances than existing NOCS datasets (NOCS-Real275, Wild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS prediction model (NOCSformer) that can predict accurate NOCS, instance masks and poses from 2D object detections across diverse classes. It is the first NOCS model that can generalize to a broad range of classes when prompted with 2D boxes. We evaluate our model on the task of 3D oriented bounding box prediction, where it achieves comparable results to state-of-the-art 3D detection methods such as Cube R-CNN. Unlike other 3D detection methods, our model also provides detailed and accurate 3D object shape and segmentation. We propose a novel benchmark for the task of NOCS prediction based on OmniNOCS, which we hope will serve as a useful baseline for future work in this area.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">OmniNOCSECCV2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krishnan, Akshay and Kundu, Abhijit and Maninis, Kevis-Kokitsi and Hays, James and Brown, Matthew}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://nileshkulkarni.github.io/nifty/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/nifty/nifty_teaser.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="KulkarniCVPR2024Nifty" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</div>
          <!-- Author -->
          <div class="author">
<a href="https://nileshkulkarni.github.io/" target="_blank" rel="noopener noreferrer">Nilesh Kulkarni</a>,Â <a href="https://davrempe.github.io/" target="_blank" rel="noopener noreferrer">Davis Rempe</a>,Â <a href="https://www.kylegenova.com/" target="_blank" rel="noopener noreferrer">Kyle Genova</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://web.eecs.umich.edu/~justincj/" target="_blank" rel="noopener noreferrer">Justin Johnson</a>,Â <a href="https://cs.nyu.edu/~fouhey/" target="_blank" rel="noopener noreferrer">David Fouhey</a>,Â <a href="https://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2307.07511" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kulkarni_NIFTY_Neural_Object_Interaction_Fields_for_Guided_Human_Motion_Synthesis_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://nileshkulkarni.github.io/nifty/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KulkarniCVPR2024Nifty</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kulkarni, Nilesh and Rempe, Davis and Genova, Kyle and Kundu, Abhijit and Johnson, Justin and Fouhey, David and Guibas, Leonidas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://m-niemeyer.github.io/nerfmeshing/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/nerf_meshing/animation_lego.gif"></a></div>

        <!-- Entry bib key -->
        <div id="NeRFMeshing3DV2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes</div>
          <!-- Author -->
          <div class="author">
<a href="https://scholar.google.com/citations?hl=en&amp;user=eQ0om98AAAAJ" target="_blank" rel="noopener noreferrer">Marie-Julie Rakotosaona</a>,Â <a href="https://scholar.google.de/citations?user=bERItx8AAAAJ&amp;h" target="_blank" rel="noopener noreferrer">Fabian Manhardt</a>,Â Diego Martin Arroyo,Â <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Michael Niemeyer</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://federicotombari.github.io/" target="_blank" rel="noopener noreferrer">Federico Tombari.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 3DV</em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.09431" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2303.09431.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://m-niemeyer.github.io/nerfmeshing/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NeRFMeshing3DV2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rakotosaona, Marie-Julie and Manhardt, Fabian and Arroyo, Diego Martin and Niemeyer, Michael and Kundu, Abhijit and Tombari, Federico}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{3DV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://jetd1.github.io/nerflets-web/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/nerflets/nerflets_teaser.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="ZhangCVPR2023Nerflets" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision</div>
          <!-- Author -->
          <div class="author">
<a href="https://scholar.google.com/citations?user=cTGxuQQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Xiaoshuai Zhang</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>,Â <a href="https://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>,Â <a href="https://cseweb.ucsd.edu/~haosu/" target="_blank" rel="noopener noreferrer">Hao Su</a>,Â <a href="https://www.kylegenova.com/" target="_blank" rel="noopener noreferrer">Kyle Genova.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.03361" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2303.03361.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://jetd1.github.io/nerflets-web/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We address efficient and structure-aware 3D scene representation from images. Nerflets are our key contribution â€“ a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, orientation, and extent, within which it contributes to panoptic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor environments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) allow the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ZhangCVPR2023Nerflets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Xiaoshuai and Kundu, Abhijit and Funkhouser, Thomas and Guibas, Leonidas and Su, Hao and Genova, Kyle}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/nf_diffusion/nf_diffusion_teaser.gif">
</div>

        <!-- Entry bib key -->
        <div id="NFDiffusionICLR2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Learning a Diffusion Prior for NeRFs</div>
          <!-- Author -->
          <div class="author">
<a href="https://www.guandaoyang.com/" target="_blank" rel="noopener noreferrer">Guandao Yang</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>,Â <a href="https://jonbarron.info/" target="_blank" rel="noopener noreferrer">Jonathan T. Barron</a>,Â <a href="https://cs.stanford.edu/~poole/" target="_blank" rel="noopener noreferrer">Ben Poole.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICLR Workshop</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2304.14473" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2304.14473.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NFDiffusionICLR2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Learning a Diffusion Prior for NeRFs}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Guandao and Kundu, Abhijit and Guibas, Leonidas and Barron, Jonathan T. and Poole, Ben}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR Workshop}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/pnf"><img class="img-fluid rounded" src="/assets/img/pnf/kitti360_nvs50_train02.webp"></a></div>

        <!-- Entry bib key -->
        <div id="KunduCVPR2022PNF" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.kylegenova.com/" target="_blank" rel="noopener noreferrer">Kyle Genova</a>,Â <a href="https://scholar.google.com/citations?user=2eADYP8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Xiaoqi Yin</a>,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi</a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru</a>,Â <a href="https://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>,Â <a href="https://taiya.github.io/" target="_blank" rel="noopener noreferrer">Andrea Tagliasacchi</a>,Â <a href="https://dellaert.github.io/" target="_blank" rel="noopener noreferrer">Frank Dellaert</a>,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2205.04334" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2205.04334.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="/projects/pnf" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff).  Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direction, and time and outputs density and radiance.  The background stuff is represented by a similar MLP that additionally outputs semantic labels. Each object MLPs are instance-specific and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-specific priors incorporated via meta-learned initialization. Our model builds a panoptic radiance field representation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly optimize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color images and pseudo-supervision from predicted semantic segmentations. During experiments with real-world dynamic scenes, we find that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmentation, 3D scene editing, and multiview depth prediction.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduCVPR2022PNF</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Genova, Kyle and Yin, Xiaoqi and Fathi, Alireza and Pantofaru, Caroline and Guibas, Leonidas and Tagliasacchi, Andrea and Dellaert, Frank and Funkhouser, Thomas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://kubric.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/kubric/KLEVR.gif"></a></div>

        <!-- Entry bib key -->
        <div id="KubricCVPR2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Kubric: A scalable dataset generator</div>
          <!-- Author -->
          <div class="author">Klaus Greff,Â Francois Belletti,Â Lucas Beyer,Â Carl Doersch,Â Yilun Du,Â Daniel Duckworth,Â David J Fleet,Â Dan Gnanapragasam,Â Florian Golemo,Â Charles Herrmann,Â Thomas Kipf,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â Dmitry Lagun,Â Issam Laradji,Â Hsueh-Ti (Derek) Liu,Â Henning Meyer,Â Yishu Miao,Â Derek Nowrouzezahrai,Â Cengiz Oztireli,Â Etienne Pot,Â Noha Radwan,Â Daniel Rebain,Â Sara Sabour,Â Mehdi S. M. Sajjadi,Â Matan Sela,Â Vincent Sitzmann,Â Austin Stone,Â Deqing Sun,Â Suhani Vora,Â Ziyu Wang,Â Tianhao Wu,Â Kwang Moo Yi,Â Fangcheng Zhong,Â <a href="https://taiya.github.io/" target="_blank" rel="noopener noreferrer">Andrea Tagliasacchi.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2203.03570" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2203.03570.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://github.com/google-research/kubric" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code <i class="fab fa-github"></i></a>
            <a href="https://kubric.readthedocs.io/en/latest/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Data is the driving force of machine learning, with the amount and quality of training data often being more important for the performance of a system than architecture and training details. But collecting, processing and annotating real data at scale is difficult, expensive, and frequently raises additional privacy, fairness and legal concerns. Synthetic data is a powerful tool with the potential to address these shortcomings: 1) it is cheap 2) supports rich ground-truth annotations 3) offers full control over data and 4) can circumvent or mitigate problems regarding bias, privacy and licensing. Unfortunately, software tools for effective data generation are less mature than those for architecture design and training, which leads to fragmented generation efforts. To address these problems we introduce Kubric, an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes, with rich annotations, and seamlessly scales to large jobs distributed over thousands of machines, and generating TBs of data. We demonstrate the effectiveness of Kubric by presenting a series of 13 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation. We release Kubric, the used assets, all of the generation code, as well as the rendered datasets for reuse and modification..</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KubricCVPR2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Greff, Klaus and Belletti, Francois and Beyer, Lucas and Doersch, Carl and Du, Yilun and Duckworth, Daniel and Fleet, David J and Gnanapragasam, Dan and Golemo, Florian and Herrmann, Charles and Kipf, Thomas and Kundu, Abhijit and Lagun, Dmitry and Laradji, Issam and Liu, Hsueh-Ti (Derek) and Meyer, Henning and Miao, Yishu and Nowrouzezahrai, Derek and Oztireli, Cengiz and Pot, Etienne and Radwan, Noha and Rebain, Daniel and Sabour, Sara and Sajjadi, Mehdi S. M. and Sela, Matan and Sitzmann, Vincent and Stone, Austin and Sun, Deqing and Vora, Suhani and Wang, Ziyu and Wu, Tianhao and Yi, Kwang Moo and Zhong, Fangcheng and Tagliasacchi, Andrea}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kubric: A scalable dataset generator}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/im2nerf/im2nerf.png">
</div>

        <!-- Entry bib key -->
        <div id="Mi2022im2nerf" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">im2nerf: Image to Neural Radiance Field in the Wild</div>
          <!-- Author -->
          <div class="author">
<a href="http://people.csail.mit.edu/lumi/" target="_blank" rel="noopener noreferrer">Lu Mi</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener noreferrer">David Ross</a>,Â <a href="https://dellaert.github.io/" target="_blank" rel="noopener noreferrer">Frank Dellaert</a>,Â <a href="https://www.cs.cornell.edu/~snavely/" target="_blank" rel="noopener noreferrer">Noah Snavely</a>,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2209.04061" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2209.04061.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We propose im2nerf, a learning framework that predicts a continuous neural object representation given a single input image in the wild, supervised by only segmentation output from off-the-shelf recognition methods. The standard approach to constructing neural radiance fields takes advantage of multi-view consistency and requires many calibrated views of a scene, a requirement that cannot be satisfied when learning on large-scale image data in the wild. We take a step towards addressing this shortcoming by introducing a model that encodes the input image into a disentangled object representation that contains a code for object shape, a code for object appearance, and an estimated camera pose from which the object image is captured. Our model conditions a NeRF on the predicted object representation and uses volume rendering to generate images from novel views. We train the model end-to-end on a large collection of input images. As the model is only provided with single-view images, the problem is highly under-constrained. Therefore, in addition to using a reconstruction loss on the synthesized input view, we use an auxiliary adversarial loss on the novel rendered views. Furthermore, we leverage object symmetry and cycle camera pose consistency. We conduct extensive quantitative and qualitative experiments on the ShapeNet dataset as well as qualitative experiments on Open Images dataset. We show that in all cases, im2nerf achieves the state-of-the-art performance for novel view synthesis from a single-view unposed image in the wild.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mi2022im2nerf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{im2nerf: Image to Neural Radiance Field in the Wild}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mi, Lu and Kundu, Abhijit and Ross, David and Dellaert, Frank and Snavely, Noah and Fathi, Alireza}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/2d3dnet_thumnail.png">
</div>

        <!-- Entry bib key -->
        <div id="2D3DNet3DV2021" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Semantic Segmentation with only 2D Image Supervision</div>
          <!-- Author -->
          <div class="author">
<a href="https://www.kylegenova.com/" target="_blank" rel="noopener noreferrer">Kyle Genova</a>,Â <a href="https://scholar.google.com/citations?user=2eADYP8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Xiaoqi Yin</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru</a>,Â Forrester Cole,Â Avneesh Sud,Â Brian Brewington,Â Brian Shucker,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 3DV</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2110.11325" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2110.11325.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the recent growth of urban mapping and autonomous driving efforts, there has been an explosion of raw 3D data collected from terrestrial platforms with lidar scanners and color cameras. However, due to high labeling costs, ground-truth 3D semantic segmentation annotations are limited in both quantity and geographic diversity, while also being difficult to transfer across sensors. In contrast, large image collections with ground-truth semantic segmentations are readily available for diverse sets of scenes. In this paper, we investigate how to use only those labeled 2D image collections to supervise training 3D semantic segmentation models. Our approach is to train a 3D model from pseudo-labels derived from 2D semantic image segmentations using multiview fusion. We address several novel issues with this approach, including how to select trusted pseudo-labels, how to sample 3D scenes with rare object categories, and how to decouple input features from 2D images from pseudo-labels during training. The proposed network architecture, 2D3DNet, achieves significantly better performance (+6.2-11.4 mIoU) than baselines during experiments on a new urban dataset with lidar and images captured in 20 cities across 5 continents.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">2D3DNet3DV2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantic Segmentation with only 2D Image Supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Genova, Kyle and Yin, Xiaoqi and Kundu, Abhijit and Pantofaru, Caroline and Cole, Forrester and Sud, Avneesh and Brewington, Brian and Shucker, Brian and Funkhouser, Thomas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{3DV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/multiview_segmentation"><img class="img-fluid rounded" src="/assets/img/multiview_segmentation/virtualMVFusion_thumbnail.gif"></a></div>

        <!-- Entry bib key -->
        <div id="KunduECCV2020VirtualMVFusion" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Virtual Multi-view Fusion for 3D Semantic Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://scholar.google.com/citations?user=2eADYP8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Xiaoqi Yin</a>,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi</a>,Â <a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener noreferrer">David Ross</a>,Â Brian Brewington,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ECCV</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2007.13138" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv <i class="ai ai-arxiv ai-1x"></i></a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2007.13138.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="/projects/multiview_segmentation" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and recent 3D convolution approaches.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduECCV2020VirtualMVFusion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Virtual Multi-view Fusion for 3D Semantic Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Yin, Xiaoqi and Fathi, Alireza and Ross, David and Brewington, Brian and Funkhouser, Thomas and Pantofaru, Caroline}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-58586-0_31}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-030-58586-0_31}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-030-58586-0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/pillar_detection_eccv2020_thumbnail.png">
</div>

        <!-- Entry bib key -->
        <div id="WangECCV2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Pillar-based Object Detection for Autonomous Driving</div>
          <!-- Author -->
          <div class="author">
<a href="https://yuewang.xyz/" target="_blank" rel="noopener noreferrer">Yue Wang</a>,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener noreferrer">David Ross</a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru</a>,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>,Â Justin Solomon.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ECCV</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2007.10323.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://github.com/WangYueFt/pillar-od" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code <i class="fab fa-github"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a simple and flexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to fix the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multi-view feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the final prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while significantly improving upon state-of-the-art.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">WangECCV2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yue and Fathi, Alireza and Kundu, Abhijit and Ross, David and Pantofaru, Caroline and Funkhouser, Thomas and Solomon, Justin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pillar-based Object Detection for Autonomous Driving}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-58542-6_2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-030-58542-6_2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://sites.google.com/view/lstm-3d-detection/home" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/misc/lstm_detection_eccv2020_thumbnail.png"></a></div>

        <!-- Entry bib key -->
        <div id="HuangECCV2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds</div>
          <!-- Author -->
          <div class="author">Rui Huang,Â Wanyue Zhang,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru</a>,Â <a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener noreferrer">David Ross</a>,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ECCV</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2007.12392.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://sites.google.com/view/lstm-3d-detection/home" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Detecting objects in 3D LiDAR data is a core technology for autonomous driving and other robotics applications. Although LiDAR data is acquired over time, most of the 3D object detection algorithms propose object bounding boxes independently for each frame and neglect the useful information available in the temporal domain. To address this problem, in this paper we propose a sparse LSTM-based multi-frame 3d object detection algorithm. We use a U-Net style 3D sparse convolution network to extract features for each frameâ€™s LiDAR point-cloud. These features are fed to the LSTM module together with the hidden and memory features from last frame to predict the 3d objects in the current frame as well as hidden and memory features that are passed to the next frame. Experiments on the Waymo Open Dataset show that our algorithm outperforms the traditional frame by frame approach by 7.5% mAP@0.7 and other multi-frame approaches by 1.2% while using less memory and computation per frame. To the best of our knowledge, this is the first work to use an LSTM for 3D object detection in sparse point clouds.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HuangECCV2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Rui and Zhang, Wanyue and Funkhouser, Thomas and Kundu, Abhijit and Pantofaru, Caroline and Ross, David and Fathi, Alireza}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-58523-5_16}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-030-58523-5_16}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=5M2cbMbBJ2U}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="https://cs.stanford.edu/people/jingweih/papers/advtex/" target="_blank" rel="noopener noreferrer"><img class="img-fluid rounded" src="/assets/img/misc/ato_thumbnail.png"></a></div>

        <!-- Entry bib key -->
        <div id="HuangCVPR2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Adversarial Texture Optimization from RGB-D Scans</div>
          <!-- Author -->
          <div class="author">
<a href="https://cs.stanford.edu/people/jingweih/" target="_blank" rel="noopener noreferrer">Jingwei Huang</a>,Â Justus Thies,Â Angela Dai,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â Chiyu Max Jiang,Â <a href="https://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>,Â <a href="https://niessnerlab.org/" target="_blank" rel="noopener noreferrer">Matthias NieÃŸner</a>,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2003.08400.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://github.com/hjwdzh/AdversarialTexture" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code <i class="fab fa-github"></i></a>
            <a href="https://cs.stanford.edu/people/jingweih/papers/advtex/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts. In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors. The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments. Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism. We train the discriminator by providing as â€˜realâ€™ examples pairs of input views and their misaligned versions â€“ so that the learned adversarial loss will tolerate errors from the scans. Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art. Our code is publicly available with video demonstration.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HuangCVPR2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Jingwei and Thies, Justus and Dai, Angela and Kundu, Abhijit and Jiang, Chiyu Max and Guibas, Leonidas and NieÃŸner, Matthias and Funkhouser, Thomas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adversarial Texture Optimization from RGB-D Scans}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPR42600.2020.00163}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/CVPR42600.2020.00163}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=52xlRn0ESek}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/dops_thumbnail.png">
</div>

        <!-- Entry bib key -->
        <div id="NajibiCVPR2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes</div>
          <!-- Author -->
          <div class="author">
<a href="https://www.mahyarnajibi.com/" target="_blank" rel="noopener noreferrer">Mahyar Najibi</a>,Â Guangda Lai,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â Zhichao Lu,Â Vivek Rathod,Â <a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>,Â <a href="https://www.linkedin.com/in/carolinepantofaru/" target="_blank" rel="noopener noreferrer">Caroline Pantofaru</a>,Â <a href="http://www.cs.toronto.edu/~dross/" target="_blank" rel="noopener noreferrer">David Ross</a>,Â Larry S. Davis,Â <a href="https://www.alirezafathi.org/" target="_blank" rel="noopener noreferrer">Alireza Fathi.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2004.01170.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We propose DOPS, a fast single-stage 3D object detection method for LIDAR data. Previous methods often make domain-specific design decisions, for example projecting points into a bird-eye view image in autonomous driving scenarios. In contrast, we propose a general-purpose method that works on both indoor and outdoor scenes. The core novelty of our method is a fast, single-pass architecture that both detects objects in 3D and estimates their shapes. 3D bounding box parameters are estimated in one pass for every point, aggregated through graph convolutions, and fed into a branch of the network that predicts latent codes representing the shape of each detected object. The latent shape space and shape decoder are learned on a synthetic dataset and then used as supervision for the end-to-end training of the 3D object detection pipeline. Thus our model is able to extract shapes without access to ground-truth shape information in the target dataset. During experiments, we find that our proposed method achieves state-of-the-art results by Â 5% on object detection in ScanNet scenes, and it gets top results by 3.4% in the Waymo Open Dataset, while reproducing the shapes of detected cars.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NajibiCVPR2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Najibi, Mahyar and Lai, Guangda and Kundu, Abhijit and Lu, Zhichao and Rathod, Vivek and Funkhouser, Thomas and Pantofaru, Caroline and Ross, David and Davis, Larry S. and Fathi, Alireza}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPR42600.2020.01193}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/CVPR42600.2020.01193}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=8UWqxqnXCyo}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/3D-RCNN"><img class="img-fluid rounded" src="/assets/img/3drcnn/3drcnn_thumbnail.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="3DRCNN_CVPR2018" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.biostat.wisc.edu/~yli/" target="_blank" rel="noopener noreferrer">Yin Li</a>,Â <a href="https://rehg.org" target="_blank" rel="noopener noreferrer">James M. Rehg.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/3DRCNN_CVPR18.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="/projects/3D-RCNN" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">3DRCNN_CVPR2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Li, Yin and Rehg, James M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPR.2018.00375}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/CVPR.2018.00375}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/ChongICDL2017_thumbnail.png">
</div>

        <!-- Entry bib key -->
        <div id="ChongICDL2017" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Visual 3D Tracking of Child-Adult Social Interactions</div>
          <!-- Author -->
          <div class="author">Eunji Chong,Â Audrey Southerland,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â Rebecca M Jones,Â Agata Rozga,Â <a href="https://rehg.org" target="_blank" rel="noopener noreferrer">James M. Rehg.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/ChongICDL2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We describe an approach to continuously capture childrenâ€™s 3D head pose and location during a tabletop social interaction with an adult examiner. Our approach, called face plus context, utilizes a fixed room camera in conjunction with a head-worn camera on the examiner to simultaneously capture the childâ€™s face along with the toys and social partners that provide context. Our system performs head tracking and pose estimation along with multi-target tracking to provide 3D localization and disambiguate identity. We evaluated our method on a dataset of 16 children, including both typically developing and autistic children. We present encouraging results for measuring childrenâ€™s social behaviors, along with validation results using an IMU.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ChongICDL2017</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chong, Eunji and Southerland, Audrey and Kundu, Abhijit and Jones, Rebecca M and Rozga, Agata and Rehg, James M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual 3D Tracking of Child-Adult Social Interactions}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/DEVLRN.2017.8329835}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/DEVLRN.2017.8329835}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/fso"><img class="img-fluid rounded" src="/assets/img/fso/fso_thumbnail.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="KunduCVPR2016VideoFSO" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Feature Space Optimization for Semantic Video Segmentation</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://vibhav-vineet.github.io/" target="_blank" rel="noopener noreferrer">Vibhav Vineet</a>,Â <a href="https://vladlen.info/" target="_blank" rel="noopener noreferrer">Vladlen Koltun.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In CVPR</em> 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/VideoFSO_CVPR16.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="https://bitbucket.org/infinitei/videoparsing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code <i class="fab fa-github"></i></a>
            <a href="/assets/pdf/VideoFSO_CVPR16_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides <i class="fas fa-file-powerpoint"></i></a>
            <a href="/projects/fso" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present an approach to long-range spatio-temporal regularization in semantic video segmentation. Temporal regularization in video is challenging because both the camera and the scene may be in motion. Thus Euclidean distance in the space-time volume is not a good proxy for correspondence. We optimize the mapping of pixels to a Euclidean feature space so as to minimize distances between corresponding points. Structured prediction is performed by a dense CRF that operates on the optimized features. Experimental results demonstrate that the presented approach increases the accuracy and temporal consistency of semantic video segmentation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduCVPR2016VideoFSO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Feature Space Optimization for Semantic Video Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Vineet, Vibhav and Koltun, Vladlen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPR.2016.345}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/CVPR.2016.345}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/HauerICRA2015_thumbnail.gif">
</div>

        <!-- Entry bib key -->
        <div id="HauerICRA2015" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Multi-scale perception and path planning on probabilistic obstacle maps</div>
          <!-- Author -->
          <div class="author">Florian Hauer,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://rehg.org" target="_blank" rel="noopener noreferrer">James M. Rehg</a>,Â Panagiotis Tsiotras.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICRA</em> 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/Hauer_ICRA2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a path-planning algorithm that leverages a multi-scale representation of the environment. The algorithm works in n dimensions. The information of the environment is stored in a tree representing a recursive dyadic partitioning of the search space. The information used by the algorithm is the probability that a node of the tree corresponds to an obstacle in the search space. The complexity of the proposed algorithm is analyzed and its completeness is shown.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">HauerICRA2015</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-scale perception and path planning on probabilistic obstacle maps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hauer, Florian and Kundu, Abhijit and Rehg, James M. and Tsiotras, Panagiotis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA.2015.7139779}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICRA.2015.7139779}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/JointSegRec"><img class="img-fluid rounded" src="/assets/img/JointSegRec/JointSegRec_thumbnail.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="KunduECCV2014JointSegRec" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Joint Semantic Segmentation and 3D Reconstruction from Monocular Video</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://www.biostat.wisc.edu/~yli/" target="_blank" rel="noopener noreferrer">Yin Li</a>,Â <a href="https://dellaert.github.io/" target="_blank" rel="noopener noreferrer">Frank Dellaert</a>,Â <a href="https://web.engr.oregonstate.edu/~lif/" target="_blank" rel="noopener noreferrer">Fuxin Li</a>,Â <a href="https://rehg.org" target="_blank" rel="noopener noreferrer">James M. Rehg.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ECCV</em> 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/HybridSFM-ECCV2014.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="/assets/pdf/HybridSFM-ECCV2014-supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp <i class="fas fa-file-pdf"></i></a>
            <a href="/projects/JointSegRec" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present an approach for joint inference of 3D scene structure and semantic labeling for monocular video. Starting with monocular image stream, our framework produces a 3D volumetric semantic + occupancy map, which is much more useful than a series of 2D semantic label images or a sparse point cloud produced by traditional semantic segmentation and Structure from Motion(SfM) pipelines respectively. We derive a Conditional Random Field (CRF) model defined in the 3D space, that jointly infers the semantic category and occupancy for each voxel. Such a joint inference in the 3D CRF paves the way for more informed priors and constraints, which is otherwise not possible if solved separately in their traditional frameworks. We make use of class specific semantic cues that constrain the 3D structure in areas, where multiview constraints are weak. Our model comprises of higher order factors, which helps when the depth is unobservable.We also make use of class specific semantic cues to reduce either the degree of such higher order factors, or to approximately model them with unaries if possible. We demonstrate improved 3D structure and temporally consistent semantic segmentation for difficult, large scale, forward moving monocular image sequences.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduECCV2014JointSegRec</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joint Semantic Segmentation and 3D Reconstruction from Monocular Video}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Li, Yin and Dellaert, Frank and Li, Fuxin and Rehg, James M.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-319-10599-4_45}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/978-3-319-10599-4_45}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-319-10599-4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/DhimanICRA2014_thumbnail.jpg">
</div>

        <!-- Entry bib key -->
        <div id="DhimanICRA2014" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Modern MAP inference methods for accurate and fast occupancy grid mapping on higher order factor graphs</div>
          <!-- Author -->
          <div class="author">Vikas Dhiman,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://dellaert.github.io/" target="_blank" rel="noopener noreferrer">Frank Dellaert</a>,Â Jason J. Corso.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICRA</em> 2014
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/modern_map_icra2014.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Using the inverse sensor model has been popular in occupancy grid mapping. However, it is widely known that applying the inverse sensor model to mapping requires certain assumptions that are not necessarily true. Even the works that use forward sensor models have relied on methods like expectation maximization or Gibbs sampling which have been succeeded by more effective methods of maximum a posteriori (MAP) inference over graphical models. In this paper, we propose the use of modern MAP inference methods along with the forward sensor model. Our implementation and experimental results demonstrate that these modern inference methods deliver more accurate maps more efficiently than previously used methods.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DhimanICRA2014</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modern MAP inference methods for accurate and fast occupancy grid mapping on higher order factor graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dhiman, Vikas and Kundu, Abhijit and Dellaert, Frank and Corso, Jason J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA.2014.6907129}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICRA.2014.6907129}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/icra2012_thumbnail.jpg">
</div>

        <!-- Entry bib key -->
        <div id="NamdevICRA2012" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Motion Segmentation of Multiple Objects from a Freely Moving Monocular Camera</div>
          <!-- Author -->
          <div class="author">Rahul Namdev,Â 
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://robotics.iiit.ac.in/PrincipalInvestigators.html" target="_blank" rel="noopener noreferrer">K. M. Krishna</a>,Â <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">C. V. Jawahar.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICRA</em> 2012
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/icra12.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Motion segmentation is an inevitable component for mobile robotic systems such as the case with robots performing SLAM and collision avoidance in dynamic worlds. This paper proposes an incremental motion segmentation system that efficiently segments multiple moving objects and simultaneously build the map of the environment using visual SLAM modules. Multiple cues based on optical flow and two view geometry are integrated to achieve this segmentation. A dense optical flow algorithm is used for dense tracking of features. Motion potentials based on geometry are computed for each of these dense tracks. These geometric potentials along with the optical flow potentials are used to form a graph like structure. A graph based segmentation algorithm then clusters together nodes of similar potentials to form the eventual motion segments. Experimental results of high quality segmentation on different publicly available datasets demonstrate the effectiveness of our method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NamdevICRA2012</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Motion Segmentation of Multiple Objects from a Freely Moving Monocular Camera}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Namdev, Rahul and Kundu, Abhijit and Krishna, K. M. and Jawahar, C. V.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA.2012.6224800}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICRA.2012.6224800}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3"><a href="/projects/mvslam"><img class="img-fluid rounded" src="/assets/img/mvslam/mvslam-thumbnail.jpg"></a></div>

        <!-- Entry bib key -->
        <div id="KunduICCV2011" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Realtime Multibody Visual SLAM with a Smoothly Moving Monocular Camera</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://robotics.iiit.ac.in/PrincipalInvestigators.html" target="_blank" rel="noopener noreferrer">K. M. Krishna</a>,Â <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">C. V. Jawahar.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICCV</em> 2011
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/abhijit_etal_iccv2011.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
            <a href="/assets/pdf/abhijit_etal_iccv2011_supp.pdf" class="btn btn-sm z-depth-0" role="button">Supp <i class="fas fa-file-pdf"></i></a>
            <a href="/projects/mvslam" class="btn btn-sm z-depth-0" role="button">Website <i class="fas fa-home"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper presents a realtime, incremental multibody visual SLAM system that allows choosing between full 3D reconstruction or simply tracking of the moving objects. Motion reconstruction of dynamic points or objects from a monocular camera is considered very hard due to well known problems of observability. We attempt to solve the problem with a Bearing only Tracking (BOT) and by integrating multiple cues to avoid observability issues. The BOT is accomplished through a particle filter, and by integrating multiple cues from the reconstruction pipeline. With the help of these cues, many real world scenarios which are considered unobservable with a monocular camera is solved to reasonable accuracy. This enables building of a unified dynamic 3D map of scenes involving multiple moving objects. Tracking and reconstruction is preceded by motion segmentation and detection which makes use of efficient geometric constraints to avoid difficult degenerate motions, where objects move in the epipolar plane. Results reported on multiple challenging real world image sequences verify the efficacy of the proposed framework.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduICCV2011</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Realtime Multibody Visual SLAM with a Smoothly Moving Monocular Camera}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Krishna, K. M. and Jawahar, C. V.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2011}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCV.2011.6126482}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/ICCV.2011.6126482}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/KunduICVGIP2010_thumbnail.jpg">
</div>

        <!-- Entry bib key -->
        <div id="KunduICVGIP2010" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Realtime Motion Segmentation based Multibody Visual SLAM</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://robotics.iiit.ac.in/PrincipalInvestigators.html" target="_blank" rel="noopener noreferrer">K. M. Krishna</a>,Â <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">C. V. Jawahar.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ICVGIP</em> 2010
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/abhijit_etal_icvgip2010.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Motion segmentation is an inevitable component for mobile robotic systems such as the case with robots performing SLAM and collision avoidance in dynamic worlds. This paper proposes an incremental motion segmentation system that efficiently segments multiple moving objects and simultaneously build the map of the environment using visual SLAM modules. Multiple cues based on optical flow and two view geometry are integrated to achieve this segmentation. A dense optical flow algorithm is used for dense tracking of features. Motion potentials based on geometry are computed for each of these dense tracks. These geometric potentials along with the optical flow potentials are used to form a graph like structure. A graph based segmentation algorithm then clusters together nodes of similar potentials to form the eventual motion segments. Experimental results of high quality segmentation on different publicly available datasets demonstrate the effectiveness of our method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduICVGIP2010</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Realtime Motion Segmentation based Multibody Visual SLAM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Krishna, K. M. and Jawahar, C. V.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICVGIP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2010}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/1924559.1924593}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://doi.acm.org/10.1145/1924559.1924593}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/KunduROBIO2010_thumbnail.jpg">
</div>

        <!-- Entry bib key -->
        <div id="KunduROBIO2010" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Realtime Moving Object Detection from a Freely moving Monocular Camera</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank" rel="noopener noreferrer">C. V. Jawahar</a>,Â <a href="https://robotics.iiit.ac.in/PrincipalInvestigators.html" target="_blank" rel="noopener noreferrer">K. M. Krishna.</a>
                  
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ROBIO</em> 2010
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/abhijit_etal_robio2010.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Detection of moving objects is a key component in mobile robotic perception and understanding of the environment. In this paper, we describe a realtime independent motion detection algorithm for this purpose. The method is robust and is capable of detecting difficult degenerate motions, where the moving objects is followed by a moving camera in the same direction. This robustness is attributed to the use of efficient geometric constraints and a probability framework which propagates the uncertainty in the system. The proposed independent motion detection framework integrates seamlessly with existing visual SLAM solutions. The system consists of multiple modules which are tightly coupled so that one module benefits from another. The integrated system can simultaneously detect multiple moving objects in realtime from a freely moving monocular camera.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduROBIO2010</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Realtime Moving Object Detection from a Freely moving Monocular Camera}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Jawahar, C. V. and Krishna, K. M.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ROBIO}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2010}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ROBIO.2010.5723575}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://doi.acm.org/10.1109/ROBIO.2010.5723575}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2009</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">

        <!-- Create website_entry  -->
        

        <!-- Create teaser_entry  -->
        


        <!-- Entry teaser -->
        <div class="col-sm-3">
            <img class="img-fluid rounded" src="/assets/img/misc/iros2009_thumbnail.jpg">
</div>

        <!-- Entry bib key -->
        <div id="KunduIROS2009" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Moving Object Detection by Multi-View Geometric Techniques from a Single Camera Mounted Robot</div>
          <!-- Author -->
          <div class="author">
                  <a href=""><b>Abhijit Kundu</b></a>,Â <a href="https://robotics.iiit.ac.in/PrincipalInvestigators.html" target="_blank" rel="noopener noreferrer">K. M. Krishna</a>,Â J. Sivaswamy.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IROS</em> 2009
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="/assets/pdf/abhijit_etal_iros2009.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file-pdf"></i></a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The ability to detect, and track multiple moving objects like person and other robots, is an important prerequisite for mobile robots working in dynamic indoor environments. We approach this problem by detecting independently moving objects in image sequence from a monocular camera mounted on a robot. We use multi-view geometric constraints to classify a pixel as moving or static. The first constraint, we use, is the epipolar constraint which requires images of static points to lie on the corresponding epipolar lines in subsequent images. In the second constraint, we use the knowledge of the robot motion to estimate a bound in the position of image pixel along the epipolar line. This is capable of detecting moving objects followed by a moving camera in the same direction, a so-called degenerate configuration where the epipolar constraint fails. To classify the moving pixels robustly, a Bayesian framework is used to assign a probability that the pixel is stationary or dynamic based on the above geometric properties and the probabilities are updated when the pixels are tracked in subsequent images. The same framework also accounts for the error in estimation of camera motion. Successful and repeatable detection and pursuit of people and other moving objects in realtime with a monocular camera mounted on the Pioneer 3DX, in a cluttered environment confirms the efficacy of the method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KunduIROS2009</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Moving Object Detection by Multi-View Geometric Techniques from a Single Camera Mounted Robot}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kundu, Abhijit and Krishna, K. M. and Sivaswamy, J.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2009}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS.2009.5354227}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://doi.acm.org/10.1109/IROS.2009.5354227}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container text-center">
        Â© Copyright 2024 Abhijit Kundu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-4839252-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-4839252-4');
  </script>
  </body>
</html>

